{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from get_embedding_function import get_embedding_function\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from contextlib import contextmanager\n",
    "import sys\n",
    "\n",
    "# Import necessary classes and functions\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        if not hasattr(sys, 'ps1'):\n",
    "            parser = argparse.ArgumentParser()\n",
    "            parser.add_argument(\"--reset\", action=\"store_true\", help=\"Reset the database.\")\n",
    "            args = parser.parse_args()\n",
    "            if args.reset:\n",
    "                print(\"Clearing Database\")\n",
    "                clear_database()\n",
    "\n",
    "        documents = load_documents()\n",
    "        chunks = split_documents(documents)\n",
    "        add_to_chroma(chunks)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main: {e}\")\n",
    "\n",
    "def load_documents():\n",
    "    try:\n",
    "        print(\"Loading documents...\")\n",
    "        document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "        docs = document_loader.load()\n",
    "        print(f\"Loaded {len(docs)} documents\")\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading documents: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_documents(documents):\n",
    "    try:\n",
    "        print(\"Splitting documents...\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,\n",
    "            chunk_overlap=80,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"Created {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error splitting documents: {e}\")\n",
    "        return []\n",
    "\n",
    "def add_to_chroma(chunks):\n",
    "    try:\n",
    "        print(\"Adding chunks to Chroma...\")\n",
    "        db = Chroma(persist_directory=CHROMA_PATH, embedding_function=get_embedding_function())\n",
    "        chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "        existing_items = db.get(include=[])\n",
    "        existing_ids = set(existing_items['ids'])\n",
    "\n",
    "        new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "        if new_chunks:\n",
    "            print(f\"Adding {len(new_chunks)} new chunks to the database\")\n",
    "            # Add chunks in smaller batches\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(new_chunks), batch_size):\n",
    "                batch = new_chunks[i:i+batch_size]\n",
    "                db.add_documents(batch)\n",
    "                print(f\"Added batch {i//batch_size + 1}\")\n",
    "            db.persist()\n",
    "        else:\n",
    "            print(\"No new chunks to add.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding to Chroma: {e}\")\n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "    try:\n",
    "        print(\"Calculating chunk IDs...\")\n",
    "        last_page_id = None\n",
    "        current_chunk_index = 0\n",
    "        for chunk in chunks:\n",
    "            source = chunk.metadata.get(\"source\")\n",
    "            page = chunk.metadata.get(\"page\")\n",
    "            current_page_id = f\"{source}:{page}\"\n",
    "            current_chunk_index = current_chunk_index + 1 if current_page_id == last_page_id else 0\n",
    "            chunk.metadata[\"id\"] = f\"{current_page_id}:{current_chunk_index}\"\n",
    "            last_page_id = current_page_id\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating chunk IDs: {e}\")\n",
    "        return []\n",
    "\n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "        print(\"Database cleared\")\n",
    "\n",
    "def get_embedding_function():\n",
    "    try:\n",
    "        print(\"Initializing embedding function...\")\n",
    "        embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing embedding function: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmet\\Desktop\\AI-TRIES\\venv\\lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 documents\n",
      "Splitting documents...\n",
      "Created 33 chunks\n",
      "Adding chunks to Chroma...\n",
      "Initializing embedding function...\n",
      "Calculating chunk IDs...\n",
      "Adding 33 new chunks to the database\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template = \"\"\"\n",
    "You are an AI named okaygpt and you are working for Mercedes Benz Buses and Truck.\n",
    "Answer the question based on the context below and the chat history. If you can't \n",
    "answer the question based on the given information, reply 'Sorry, I don't know'.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Chat History: {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_qa_chain():\n",
    "    # Initialize embeddings and model\n",
    "    embeddings = get_embedding_function()\n",
    "    model = Ollama(model='llama3', num_ctx=9000, base_url='http://localhost:11434')\n",
    "\n",
    "    # Load the Chroma database\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "\n",
    "    # Create a retriever from the Chroma database\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    # Define the RAG prompt template\n",
    "    template = \"\"\"\n",
    "    You are an AI named okaygpt and you are working for Mercedes Benz Buses and Truck.\n",
    "    Answer the question based on the context below and the chat history. If you can't \n",
    "    answer the question based on the given information, reply 'Sorry, I don't know'.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Chat History: {chat_history}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Initialize conversation memory\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key=\"answer\"\n",
    "    )\n",
    "\n",
    "    # Create the conversational retrieval chain\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        memory=memory,\n",
    "        combine_docs_chain_kwargs={\"prompt\": rag_prompt},\n",
    "         return_source_documents=True,\n",
    "        return_generated_question=False\n",
    "        \n",
    "    )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "def start_app(qa_chain):\n",
    "    message_count = 0\n",
    "    max_message = 3\n",
    "    while message_count <= max_message:\n",
    "        question = input('You: ')\n",
    "        print(f'User: {question}\\n')\n",
    "\n",
    "        if question.lower() == 'done':\n",
    "            print('Session ended.')\n",
    "            break\n",
    "        \n",
    "        # Get response from the chain\n",
    "        response = qa_chain({\"question\": question})\n",
    "        \n",
    "        print('AI:', response['answer'])\n",
    "        \n",
    "        # Print sources\n",
    "        print(\"\\nSources:\")\n",
    "        for doc in response['source_documents']:\n",
    "            print(f\"- {doc.metadata['source']} (Page {doc.metadata['page']})\")\n",
    "        \n",
    "        message_count += 1\n",
    "    \n",
    "    print('AI terminated itself!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing embedding function...\n",
      "User: who wrote information form group 1 and can you give me their numbers also\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmet\\Desktop\\AI-TRIES\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Sorry, I don't know. The context is about a project for Mercedes Benz Buses and Trucks, but there's no mention of \"Information Form Group 1\" or any specific author/number associated with it.\n",
      "\n",
      "Sources:\n",
      "- data\\Final Report.pdf (Page 6)\n",
      "- data\\Thesis_Premise.pdf (Page 5)\n",
      "- data\\Final Report.pdf (Page 9)\n",
      "- data\\Thesis_Premise.pdf (Page 1)\n",
      "User: done\n",
      "\n",
      "Session ended.\n",
      "AI terminated itself!\n"
     ]
    }
   ],
   "source": [
    "qa_chain = setup_qa_chain()\n",
    "start_app(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
